{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AI Chat Assistant\n",
    "\n",
    "**Converse with Your Data Effortlessly**\n",
    "\n",
    "AI Chat Assistant is an open-source tool that enables natural language conversations with a wide range of data sources, from documents to web links and even YouTube videos. With the power of state-of-the-art language models, including Retrieval-Augmented Generation (RAG), this tool empowers you to ask questions, extract insights, and explore your data interactively. Enjoy the convenience of a user-friendly interface and the flexibility to choose your language model, all while running efficiently on standard CPU hardware.\n",
    "\n",
    "Get ready to engage in meaningful conversations with your data. Let's get started!\n",
    "\n",
    "[![Run in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marawanxmamdouh/ConvoNerd/blob/master/convonerd_colab.ipynb)\n"
   ],
   "metadata": {
    "id": "mwKFTrP1eq8h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking GPU Information\n",
    "\n",
    "Before proceeding, ensure that you have selected the T4 GPU runtime by following these steps:\n",
    "1. Click on the \"Runtime\" option in the upper menu.\n",
    "2. Choose \"Change Runtime Type.\"\n",
    "3. Select the \"T4 GPU\" option from the menu."
   ],
   "metadata": {
    "id": "vLP3BJETY_fm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "ERPIe8j0L0el"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cloning a GitHub Repository\n"
   ],
   "metadata": {
    "id": "YOd_RQcEZDpb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sca-4lUSLrgy"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/marawanxmamdouh/ConvoNerd.git"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing a AutoGPTQ Package from a Wheel File\n",
    "\n",
    "In this code cell, we are performing two actions to install a Python package using a pre-built wheel file:\n",
    "\n",
    "1. **Downloading the Wheel File**\n",
    "\n",
    "2. **Installing the Python Package**\n",
    "\n",
    "   - `BUILD_CUDA_EXT=0` is used to indicate that CUDA extensions should not be built during the installation.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "CDLJurt1ZEyk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -q https://github.com/PanQiWei/AutoGPTQ/releases/download/v0.4.1/auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl\n",
    "!BUILD_CUDA_EXT=0 pip install -qqq auto_gptq-0.4.1+cu118-cp310-cp310-linux_x86_64.whl --progress-bar off"
   ],
   "metadata": {
    "id": "UGQC5nHqMQZz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing Poppler Utils\n"
   ],
   "metadata": {
    "id": "lljsmA3TZFIv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt-get install poppler-utils"
   ],
   "metadata": {
    "id": "UlKf856pMX-M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Determining the Execution Device (CPU or GPU)"
   ],
   "metadata": {
    "id": "C1fCcELVZFeH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ],
   "metadata": {
    "id": "PyqnqPr4Maxb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updating and Switching Python Version\n"
   ],
   "metadata": {
    "id": "Q-fznw7UaPg7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt-get update -y\n",
    "!sudo apt-get install python3.11\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1\n",
    "\n",
    "!sudo update-alternatives --config python3\n",
    "\n",
    "# Check the result version\n",
    "!python3 --version"
   ],
   "metadata": {
    "id": "k_TlKPqGTYFC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing Python Package Manager (pip) and Dependencies\n"
   ],
   "metadata": {
    "id": "EbJfmvE1aU7D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt install python3-pip\n",
    "!pip3 install -r /content/ConvoNerd/requirements.txt --ignore-installed"
   ],
   "metadata": {
    "id": "kDSyYWxyPGJs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing localtunnel\n",
    "\n",
    "Localtunnel is a handy tool that enables you to make your locally hosted web application accessible to the internet by generating a temporary public URL. This allows you to conveniently run and access your locally hosted Streamlit app on a Colab server using any web browser."
   ],
   "metadata": {
    "id": "6nhuBkazaZQm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!npm install localtunnel"
   ],
   "metadata": {
    "id": "lu2W4Yq4Mc3t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running Streamlit App and Exposing It with Localtunnel\n",
    "\n",
    "- Once you execute this cell, you will receive output similar to the following. Make sure to open the URL that ends with `loca.lt`. In this example (silent-bags-cover.loca.lt).\n",
    "- When prompted for an IP, use the IP printed in the first line of this cell. In this example (34.172.121.81).\n",
    "\n",
    "```plaintext\n",
    "/content/ConvoNerd\n",
    "34.172.121.81\n",
    "[..................] \\ fetchMetadata: sill resolveWithNewModule yargs@17.1.1 ch\n",
    "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
    "\n",
    "You can now view your Streamlit app in your browser.\n",
    "\n",
    "Network URL: http://172.28.0.12:8501\n",
    "External URL: http://34.172.121.81:8501\n",
    "\n",
    "npx: installed 22 in 4.107s\n",
    "Your URL is: https://silent-bags-cover.loca.lt\n",
    "```"
   ],
   "metadata": {
    "id": "nAdUhJ88bO_w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/ConvoNerd\n",
    "!streamlit run app.py & curl ipv4.icanhazip.com & npx localtunnel --port 8501"
   ],
   "metadata": {
    "id": "y9XO8vQcM_cs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Important Note: Model Selection\n",
    "\n",
    "This notebook is optimized to run the `TheBloke/Llama-2-7B-chat-GPTQ` model. If you intend to use a different GPTQ model go to the `ConvoNerd/conf/language_models.yaml` file and change the `gptq_model.model_name` parameter to the name of your desired model. \n",
    "\n",
    "If you intend to use a different model type, please follow these steps:\n",
    "\n",
    "1. Download the model weights file using `wget`. You can find the URLs for different models in the `ConvoNerd/models/Model Download Instructions.md` file.\n",
    "\n",
    "2. Move the downloaded model file to the `ConvoNerd/models` directory within this project's structure.\n"
   ],
   "metadata": {
    "id": "FvHeh8Mpcojo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
   ],
   "metadata": {
    "id": "_AgS8HQJM65F"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
